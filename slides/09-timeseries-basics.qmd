---
subtitle: "Applied Statistics -- A Practical Course"
title: "09-Time Series Basics"
date:   "`r Sys.Date()`"
--- 
```{r packages, echo=FALSE, include=FALSE}
library(dplyr)
library(readr)
library(ggplot2)
library(lubridate)
library(tseries)
```

# Introductory examples

## Example 1: CO2 in the atmosphere

<!-- shown code reads data directly from NOAA -->

```{r co2-maunaloa-show, eval=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: "Show the code"
library(dplyr)
library(readr)
library(ggplot2)
library(lubridate)

co2 <- read_csv("https://gml.noaa.gov/webdata/ccgg/trends/co2/co2_mm_mlo.csv", 
                skip = 40, show_col_types = FALSE)

co2 |> 
  mutate(date=as_date(paste(year, month, 15, sep="."))) |>
  ggplot(aes(date, average)) + 
  geom_line() + ylab(expression(CO[2]~"in the atmosphere (ppm)")) + 
  ggtitle("Mauna Loa Observatory, Hawaii, Monthly Averages") +
  geom_smooth()
```

<!-- slides use locally cached data -->

```{r co2-maunaloa, echo=FALSE, fig.align='center'}
co2 <- read_csv("../data/co2_mm_mlo.csv", skip=40, show_col_types = FALSE) |>
  mutate(date=as_date(paste(year, month, 15, sep=".")))

co2 |> ggplot(aes(date, average)) + 
  geom_line() + 
  ylab(expression(CO[2]~"in the atmosphere (ppm)")) + 
  ggtitle("Mauna Loa Observatory, Hawaii, Monthly Averages") +
  geom_smooth()
```


Data source:

@keeling_exchanges_2001, @tans_maunaloa_2023, [https://gml.noaa.gov/ccgg/trends/data.html](https://gml.noaa.gov/ccgg/trends/data.html)

<!--
https://gml.noaa.gov/ccgg/trends/data.html
License of the data: CC BY 4.0 (see https://scrippsco2.ucsd.edu/data/atmospheric_co2/primary_mlo_co2_record.html)
-->

## Example 2: Monthly mean air temperature

```{r airtemp-monthly, echo=TRUE}
#| code-fold: true
#| code-summary: "Show the code"
library(dplyr)
library(readr)
library(ggplot2)
library(lubridate)

tempdata <- read_csv("../data/airtemp_dresden_daily.csv") |>
  select(ZEIT, TM) |>
  rename(date = ZEIT) |>
  mutate(year = year(date), month = month(date)) |>
  group_by(year, month) |>
  summarise(temp = mean(TM)) |>
  mutate(date = date(paste(year, month, 15, sep="-")))

tempdata |>
  ggplot(aes(date, temp)) + 
  geom_line() +
  ylab("Temperature (°C)") + 
  ggtitle("Monthly mean air temperature in Dresden")

```

Data downloaded from [https://rekis.hydro.tu-dresden.de](https://rekis.hydro.tu-dresden.de) 
[@kronenberg_rekis_2021], original source Deutscher Wetterdienst, https://www.dwd.de, data modified and averaged.


## Example 3: Annual mean air temperature

```{r airtemp-annual, echo=TRUE}
#| code-fold: true
#| code-summary: "Show the code"
library("dplyr")
library("ggplot2")
library("lubridate")
library("readr")
## adapt the following line if data were downloaded:
file <- "../data/airtemp_dresden_daily.csv"
## or, uncomment to access data directly from the internet:
#file <- "https://tpetzoldt.github.io/datasets/data/airtemp_dresden_daily.csv"
read_csv(file, show_col_types = FALSE) |>
  select(ZEIT, TM) |>
  rename(date = ZEIT) |>
  mutate(year = year(date)) |>
  group_by(year) |>
  summarise(temp = mean(TM)) |>
  ggplot(aes(year, temp)) + 
  geom_line() +
  geom_smooth(method="lm") +
  ylab("Temperature (°C)") + 
  ggtitle("Annual average air temperature in Dresden")
```

[Data downloaded from [https://rekis.hydro.tu-dresden.de](https://rekis.hydro.tu-dresden.de) 
[@kronenberg_rekis_2021], original source Deutscher Wetterdienst, https://www.dwd.de, data modified and averaged.]{.small}

## Characteristics of the examples

<br>

* Observations as a function of time
* Measurements can be serially interdependent, e.g.:
    - trend
    - seasonality
* No "true" replicates

<br>

$\Rightarrow$ independency assumption of simple linear regression is violated


## Time series analysis

<br>

* Deals with development of processes in time: $x(t)$,
* Huge number of specific approaches, only a few examples can be
  presented here.


**Aims**

* Does a trend exist? (significance)
* How strong is a trend? (effect size)
* Identification of covariates (influencing factors)
* Trend, seasonality and random error (component model)
* Statistical modeling and forecasting
* Breakpoint analysis, intervention analysis, ..., and more

# Fundamental time series concepts

## Stationarity

<br>

* Time series methods have certain assumptions.
* One of the most common assumptions is **stationarity**.

<br>


**Note:** We are usually not primarily interested in stationarity itself!

$\rightarrow$ It is just "the ticket" for further analyses.

<br>


**Example 1: trend analysis**

* We test stationarity to check if a (simple) trend test would be appropriate.

**Example 2: linear models**

* Estimation of a linear trend or a breakpoint model. 
* We check residuals for stationarity to get in formation about reliability of the fitted model.

## Stationarity: a central concept in time series analysis

```{r stat-nonstat, echo=TRUE, fig.align='center'}
#| code-fold: true
#| code-summary: "Show the code"
  set.seed(123)
  x <- 1:100
  par(mfrow = c(1, 3), cex=1.4)
  plot(x, rnorm(x), type="l", main="stationary", xlab="time", ylab="x")
  plot(x, 0.01 * x + rnorm(x), type="l", main="linear trend", xlab="time", ylab="x")
  plot(x, rnorm(x, sd=seq(0.1,1,length=100)), type="l",
    main="increasing variance", xlab="time", ylab="x")
```

* Strictly or **strong stationary process**: distribution of
  $(x_{s+t})$ independent from index $s$.
* Wide-sense or **weakly stationary processes**: requires only that
  1st and 2nd moments (mean, variance and covariance) do not vary with
  time.

## Three types of basic time series

```{r gen-LTDsp}
set.seed(1237)
time <- 1:100

LSP <- 4 + rnorm(time)

TSP <- 4 + 0.2 * time + rnorm(time)

DSP <- numeric(length(time))
DSP[1] <- rnorm(1)
for (tt in time[-1])  DSP[tt] <- DSP[tt-1] + 0.2 + rnorm(1)
```


::: {.column width="32%"}

$x_t = \beta_0 + \varepsilon_t$

```{r LSP, fig.height=6, fig.width=6}
par(cex=2, las=1, mar=c(4,4,2,1))
plot(ts(LSP), ylim=c(0, 8), main="LSP", col = "forestgreen")
```


* "level stationary process"
* mean, variance and covariance constant
* example: white noise
* $=$ **stationary**

:::

::: {.column width="32%"}

$x_t = \beta_0 + \beta_1 t + \varepsilon_t$

```{r TSP, fig.height=6, fig.width=6}
par(cex=2, las=1, mar=c(4,4,2,1))
plot(ts(TSP), main="TSP", col = "red")
```



* "trend stationary process"
* linear regression model
* can be made stationary by detrending
* $\rightarrow$ **non-stationary**

:::

::: {.column width="32%"}

$x_t = x_{t-1} + c + \varepsilon_t$

```{r DSP, fig.height=6, fig.width=6}
par(cex=2, las=1, mar=c(4,4,2,1))
plot(ts(DSP), col = "red", main = "DSP")
```


* "difference stationary process" (random walk)
* can be made stationary by differencing
* $\rightarrow$ **non-stationary**

:::

## Simulated data

```{r gen-LTDsp-show, echo=TRUE}
#| code-fold: true
#| code-summary: "Show the code"

set.seed(1237)
time <- 1:100

LSP <- 4 + rnorm(time)

TSP <- 4 + 0.2 * time + rnorm(time)

DSP <- numeric(length(time))
DSP[1] <- rnorm(1)
for (tt in time[-1])  DSP[tt] <- DSP[tt-1] + 0.2 + rnorm(1)

par(mfrow=c(1,3))

plot(ts(LSP), ylim=c(0, 8), main="LSP", col = "forestgreen")
plot(ts(TSP), main="TSP", col = "red")
plot(ts(DSP), main="DSP", col = "red")
```

[The introductory examples showed additional types of time series, e.g. with seasonality.]{.small}

## Why is this important?

<br>

Simple linear models require "independence", or, more precisely: **independence of residuals**.

<br>

**But**

* time series observations can be serially dependent
* dependent data have "less value" [(contain less information than independent data)]{.gray}
* important for calculation of degrees of freedom for the sigificance tests

**Approaches**

1. measure serial dependency (autocorrelation)
2. if yes, handle autocorrelation
    * remove autocorrelation, [or]{.gray}
    * model autocorrelation

# Autocorrelation

* measures temporal dependency (memory) of observations

## Autocorrelation

<br>

```{r}
laggedMatrix <- function(x, k) {
  L <- length(x + k)
  z <- matrix(NA, nrow = L + k - 1, ncol = k)
  for (i in 1:k) {
    z[(1:L) + (i - 1), i] <- x
  }
  return(z)
}
```

**A time series**

```{r}
options(knitr.kable.NA = '')
x  <- c(1, 3, 2, 4, 5, 4, 3, 2, 8, 5, 4, 3, 6, 7)
knitr::kable(t(data.frame("x_t" = c(x, NA, NA))))
```

<br>

**Shifting to the right**

```{r}
options(knitr.kable.NA = '')
df <- data.frame(laggedMatrix(x, 3))
names(df) <- c("x_t", "x_t+1", "x_t+2")
knitr::kable(t(df))
```

<br>

**Correlation between $x_t$ and $x_{t+1}$ and $x_{t+2}$:**

```{r, echo=TRUE}
cor(df, use = "pairwise.complete.obs")[1,]
```

[In practice, autocorrelation is calculated using the `acf` function of R. 
The algorithm is somewhat different, especially the treatment of missing values, 
so results differ, especially for small samples.]{.gray}



## Autocorrelation and partial autocorrelation (ACF, PACF)

![](../img/time-series.png){fig-align="center"}

## Autocorrelation function (ACF): autocorrelogram

* ... correlation between time series and its time-shifted (= lagged) versions

* ACF considers both, direct and indirect dependency
* PACF considers only direct effects it is called **partial autocorrelation**, PACF.

```{r acf-example, fig.align='center'}
par(mfrow=c(1,2), cex=1.4)
acf(TSP, main = "acf(TSP)")
pacf(TSP, main = "pacf(TSP)")
```

# Cross-correlation (CCF)

<br>

* time-shift between two or more variables
* lag can be positive or negative


**Examples**

- CCF between air and water temperature in rivers and lakes
- CCF of discharge at two river kilometers $\rightarrow$ estimate flow velocity of a flood wave

## Delay between air and water temperature in a lake


```{r air-water-timeseries, fig.align='center'}
water <- read_csv("../data/saidenbach-water-temperature.csv", show_col_types = FALSE) |>
  mutate(Date = as.Date(Date), Year = year(Date), doy=yday(Date)) |>
  filter(Depth == 5, 2009 < Year, Year < 2016)

days <- seq(as.Date("2010-01-01"), as.Date("2015-12-31"), by = 1)

water <- approx(water$Date, water$WT, days, ties=mean, rule=2) |>
  as.data.frame() |>
  rename(Date=x, WT=y) |>
  mutate(Year = year(Date), doy=yday(Date))

air <- read.csv("../data/airtemp_dresden_daily.csv") |>
  mutate(Date = as.Date(ZEIT), Year = year(Date)) |>
  filter(2009 < Year, Year < 2016) |>
  select(Date, TX, TM, TN)

water_air <- left_join(water, air, by ="Date")

par(mar=c(4,4,1,1))
plot(TM ~ Date, data=water_air, col="red", las=1, type = "l", ylab=c("temperature (°C)"))
lines(WT ~ Date, data=water_air, col="blue", lwd=3)
legend("bottomright", lty=1, lwd=c(1, 3), col=c("red", "blue"), legend=c("air", "water"))
```

* air temperature, station Dresden-Klotsche, data from German Weather Service 
* water temperature in Saidenbach Reservoir, 5m below the surface<br>
(Data from Ecological Station Neunzehnhain), @horn_drei_2006, @paul_saidenbach_2020

## Cross correlation between air and water temperature

<br>

```{r air-water-ccf, fig.align='center'}
cc <- ccf(water_air$WT, water_air$TM, lag.max=180, main="CCF between water and air temperature",
          axes=FALSE, xlab="Lag (days)", ylab="CCF")
axis(2, las=1)
axis(1, seq(-180, 180, 30))
box()
acfmax <- cc$lag[cc$acf == max(cc$acf)]

abline(v=acfmax, col="blue", lwd=3)
abline(v=0, col="red", lty="dashed", lwd=3)

arrows(x0=0, y0=-0.2, x1=acfmax, angle=15, lwd=3)
text(acfmax, -0.2, paste(acfmax, "days"), pos=4)
legend("bottomright", lty=c("solid", "dashed"), lwd=c(1, 3), 
       col=c("red", "blue"), legend=c("air", "water"))
```

# Interpretation and removal of autocorrelation

## Typical patterns of ACF

```{r acf-patterns, echo=TRUE, fig.align='center'}
#| code-fold: true
#| code-summary: "Show the code"
par(mfrow=c(2, 2))
acf(LSP, main = "level stationary")
acf(TSP, main = "trend stationary")
acf(DSP, main = "difference stationary")

time <- seq(0, 4 * pi, length.out = 100)
periodic <- ts(cos(time) + rnorm(100, sd = 0.1))
acf(periodic, lag = 100, main = "periodic")
```

## Removal of autocorrelation

<br>

* **Trend stationary series:** trend removal

Example:

```{r, echo=TRUE, eval=FALSE}
m <- lm(TSP ~ time(TSP))
residuals(m)
```

<br>

* **Difference stationary series:** differencing

Example:

```{r, echo=TRUE, eval=TRUE}
x <- c(2, 3, 4, 3, 6, 4, 3, 8, 5, 9)
diff(x)
```

<br>

* **Periodic series:** identification and removal of seasonal components

* Methods: spectral analysis, seasonal decomposition, averaging ...


## Removal of autocorrelation II

```{r acfpatterns, echo=FALSE, fig.width=14, fig.height=6, fig.align='center'}
par(mfrow=c(2,3), cex=1, las=1)
acf(TSP, lwd=4)
acf(diff(TSP), lwd=4, col=c(1, 2, rep(1, 18)))
acf(residuals(lm(TSP ~ time(TSP))), col="#009de0", lwd=4)
acf(DSP, lwd=4)
acf(diff(DSP), col="#009de0", lwd=4)
acf(residuals(lm(DSP ~ time(DSP))), lwd=4)
```

::: {.column width="34%"}
* ACF plots of TSP and DSP
* strong autocorrelation<br> $\rightarrow$ non-stationary
:::

::: {.column width="34%"}
* differencing
* differences of DSP<br> $\rightarrow$ stationary
:::

::: {.column width="30%"}
* detrending
* residuals of TSP<br> $\rightarrow$ stationary
:::

# Statistical tests

## Test of stationarity

<br>

* Kwiatkowski-Phillips-Schmidt-Shin test (KPSS test)
* has built-in trend removal (option `null = "trend"`)



```{r kpss-hypotheses, fig.align='center', fig.width=14, fig.height=3}
TSP <- ts(TSP)
par(mfrow = c(1, 2))
plot(TSP, main="H0: level stationarity")
abline(h=mean(TSP), col="red")

plot(TSP, main="H0: linear trend")
abline(lm(TSP~time(TSP)), col="red")
```


::: {.column width="49%"}
```{r, echo=TRUE}
kpss.test(TSP)
```

$\rightarrow$ non-stationary
:::


::: {.column width="49%"}
```{r, echo = TRUE}
kpss.test(TSP, null = "Trend")
```

$\rightarrow$ stationary after trend removal

:::


## KPSS test with a difference stationary process (DSP)

<br>

```{r kpss-hypotheses2, fig.align='center', fig.width=14, fig.height=3}
DSP <- ts(DSP)
par(mfrow = c(1, 2))
plot(DSP, main="H0: level stationarity")
abline(h=mean(DSP), col="red")

plot(DSP, main="H0: linear trend")
abline(lm(DSP~time(DSP)), col="red")
```


::: {.column width="49%"}
```{r, echo=TRUE}
kpss.test(DSP)
```

$\rightarrow$ non-stationary
:::


::: {.column width="49%"}
```{r, echo = TRUE}
kpss.test(DSP, null = "Trend")
```

$\rightarrow$ trend removal does not cure non-stationarity

:::

<br>

**Task:** Check if a DSP series can be made stationary by differencing.


## Trend Tests

**Mann-Kendall trend test**

* nonparametric test
* popular in environmental sciences
* robust in case of weak autocorrelation

**Linear regression analysis**

* parametric test for linear trend
* slope gives direct information about effect size
* sensitive against autocorrelation

**Other methods**

* Sen's slope, Pettitt's Test, Cox-Stuart Test, ...

**In general**

* Trend tests are not adequate for if residuals are autocorrelated.
* [$\Rightarrow$ check stationarity (or autocorrelation) of residuals after trend removal !!!]{.red}


## Mann-Kendall-Trend-Test

<br><br>

```{r, echo=TRUE}
library("Kendall")
MannKendall(TSP)
```

$\rightarrow$ significant trend

<br>

```{r, echo=TRUE}
MannKendall(DSP)
```


$\rightarrow$ indicates significant trend, but as process is DSP, interpretation is wrong.



## Application: trend of air temperature

<br>

```{r temp-april, fig.align="center", fig.width=9, fig.height=3, echo=TRUE}
temp <- read.csv("https://tpetzoldt.github.io/datasets/data/airtemp_april.csv")
temp <- ts(temp$T, start=temp$Year[1])
plot(temp)
```


[Data: Air temperature in April, station Dresden-Klotsche (Germany), data source: Deutscher Wetterdienst (https://www.dwd.de), data aggregated and modified.]{.gray}


## Check of stationarity of residuals and trend test

<br>

Check for stationarity

```{r, echo=TRUE}
kpss.test(temp, null = "Trend")
```

<br>

Mann-Kendall-Trend-test

```{r, echo=TRUE}
library(Kendall)
MannKendall(temp)
```

<br>

* KPSS test does not reject H0 of trend stationarity
* Mann-Kendall is significant $\rightarrow$ monotonous trend

## Estimate slope of linear trend


```{r temp-trend, fig.align="center", fig.width=12, fig.height=4, echo=TRUE}
#| code-fold: true
#| code-summary: "Show the code"
plot(temp)
m <- lm(temp ~ time(temp))
abline(m, col="red")
```

```{r, echo=TRUE}
summary(m)
```

# Further reading

<br>

* @kleiber_applied_2008: a very practical book about econometrics with well understandable time series chapters

* @shumway_time_2019: a book with a time series good introduction and good balance between mathematics and R programming

* @shumway_time_2000: contains additional methods and more mathematical background

... and more in the labs and the homework project.

# Appendix

<!--
## Time series classes in R

* `ts` for regularly spaced time series,
* `zoo` irregularly spaced time series.


```{r ts-example1, echo=TRUE, fig.height=5, fig.width=12, fig.align='center'}
par(mfrow=c(1, 3), las=1)
LSP <- ts(LSP)
TSP <- ts(TSP)
DSP <- ts(DSP)

plot(LSP, main="level stationary process")
plot(TSP, main="trend stationary process")
plot(DSP, main="difference stationary process")
```
-->

## A simulation experiment

<br>

Demonstration that application of a simple linear model to a difference stationary time series leads to an increase of false positives.

<br>

**Tools**

* Define two functions to generate time series with specific properties:


```{r, echo=TRUE}
genTSP <- function(time, beta0, beta1)
  as.ts(beta0 + beta1 * time + rnorm(time))

genDSP <- function(time, c) {
  DSP <- numeric(length(time))
  DSP[1] <- rnorm(1)
  for (tt in time[-1]) DSP[tt] <- DSP[tt-1] + c + rnorm(1)
  ts(DSP)
}
```


## Count significant tests


```{r, echo=TRUE}
count.signif <- function(N, time, FUN, ...) {
  a <- 0
  for (i in 1:N) {
    x <- FUN(time, ...)
    m <- summary(lm(x  ~ time(x)))
    f <- m$fstatistic
    p.value <- pf(f[1], f[2], f[3], lower=FALSE)
    # cat("p.value", p.value, "\n")
    if (p.value < 0.05) a <- a + 1
  }
  a
}
```

**Run the experiment**

* count number of significant linear increase (using the F-value)
* if trend parameters (`beta1` an `c`) are set to zero,  `count.signif` counts false positives
* run loop 100 (or better: 1000) times for both types of time series

```{r, echo=TRUE, fig.align='center'}
Nruns <- 100 
time  <- 1:100
count.signif(N=Nruns, time=time, FUN=genTSP, beta0=0, beta1=0) / Nruns
count.signif(N=Nruns, time=time, FUN=genDSP, c=0) / Nruns
```

## Unit root test

<br>

* another method for stationarity testing
* checks whether a time series is of type "DSP".
* ADF-test (augmented Dickey-Fuller test), contained in R-package **tseries**.
* remember: a "TSP" can be made stationary by subtracting a trend.
* $\rightarrow$ This is done automatically by the test.
* **Important:** in the ADF test, stationarity is the **alternative** hypothesis!

<br>

```{r, echo=TRUE}
library(tseries)
adf.test(TSP)
adf.test(DSP)
```


## DSP: Presence of a "unit root"

<br>

* DSP series: presence of a unit root cannot be rejected,
* $\rightarrow$ non-stationary.
* But, after differencing, there are no objections against stationarity:

```{r, echo=TRUE}
adf.test(diff(DSP))
```



## References

<br>