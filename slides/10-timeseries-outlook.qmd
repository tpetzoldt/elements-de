---
subtitle: "Applied Statistics -- A Practical Course"
title: "10-Time Series Outlook"
date:   "`r Sys.Date()`"
--- 

```{r setup, echo=FALSE}
library(Kendall)
library(dplyr)
library(readr)
library(ggplot2)
library(lubridate)
```

# Time series decomposition

## Time series decomposition

<br>

**Traditional component model**


Decomposition of time series into:

1. trend component,
2. seasonal or periodic component and
3. stochastic component.

**Assumptions**

* additivity of components
* approximate normal distribution of residuals
* homogeneous variance (of residuals)

If distribution is skewed, effects are "multiplicative", or variance changes proportionally with
trend $\rightarrow$, consider transformation.

For hydrological or biomass data with right-skewed distribution, log-transformation can be helpful.

## Smoothing methods


* alternative to parametric linear or non-linear regression vs. time
* smoothers are based on moving averages

**Example:** Annual precipitation 1900-1986, Great Lakes
```{r filter1,fig=TRUE, echo=TRUE, fig.align='center'}
library(Kendall)
data(PrecipGL)
plot(PrecipGL, ylab="Precipitation (inches)")
# moving average with rectangular kernel
kernel <- rep(1, 10)  # change second value to play with bandwidth
lines(stats::filter(PrecipGL, kernel/sum(kernel)), lwd = 2, col = "blue")
```

## Trend corrected series


```{r filter2, echo=TRUE, fig.align='center'}
smooth <- stats::filter(PrecipGL, kernel/sum(kernel))
residuals <- PrecipGL - smooth
plot(residuals)
```

* Trend corrected series can be obtained by subtracting the trend.
* **Note:** use `stats::filter` to avoid potential conflict with `dplyr::filter`.

## Smoothing of data with 12monthly seasonality

* For a seasonal time series with monthly values @kleiber_applied_2008 recommend
a filter with 13 coefficients.
* Example: water level of Rio Negro 18 km upstream from its confluence with
the Amazon River. 
* The data set is contained in the package **boot** [@canty_boot_2022]:

```{r filter3,fig=TRUE, echo=TRUE, fig.align='center'}
library(boot)
data(manaus)
tsp(manaus) # shows time series properties
plot(manaus)
lines(stats::filter(manaus, c(0.5, rep(1, 11), 0.5)/12), lwd=2, col="blue")
```

## Very popular method: The LOWESS filter

```{r lowess, echo=TRUE, fig.align='center'}
#| code-fold: true
#| code-summary: "Show the code"
plot(PrecipGL)
lines(lowess(time(PrecipGL), PrecipGL, f = 2/3), lwd = 3, col = "blue")
lines(lowess(time(PrecipGL), PrecipGL, f = 1/3), lwd = 3, col = "forestgreen")
lines(lowess(time(PrecipGL), PrecipGL, f = 0.1), lwd = 3, col = "red")
```

* locally weighted polynomial regression [@cleveland_robust_1979]
* allows to adjust smoothness (smoother span `f`)
* [different functions available in **R**: `lowess` and `loess`]{.gray}


## Identification of seasonal components

<br>

**Periodic phenomena are common in nature**

* seasonal course of solar radiation and temperature
* seasonal development of vegetation 
* heartbeat of animals.

**Harmonic analysis**

* every time series can be described as a sum of sine and a cosine
functions with different periods (Fourier series)

$$
 x_t = a_0 + \sum_{p=1}^{N/2-1} \big(a_p \cos(2 \pi p t/N)
                                 +    b_p \sin(2 \pi p t/N)\big)
           +a_{N/2} \cos(\pi t), \qquad t=1 \dots N
$$

**with**

$x_t$: equidistant time series, $t$: time step, $N$: number of data, $a_p, b_p$: Fourier coefficients,<br>
$a_0 = \bar{x}$, $p$: order of the periodic component


## Formulation with single cosine term and shift $\Phi$

<br>

* equation with sine and cosine terms can be transformed in an equation with cosine terms only
* use of trigonometric addition formulas:

<br>

$$
x_t = a_0 + \sum(R_p \cdot \cos(2 \pi p t / N + \Phi_p)
$$

with:

* amplitude: $R_p = \sqrt{a_p^2 + b_p^2}$
* phase shift: $\Phi_p = \arctan(-b_p/a_p)$

<br>
see:  [https://en.wikipedia.org/wiki/List_of_trigonometric_identities](https://en.wikipedia.org/wiki/List_of_trigonometric_identities)

## Different methods to identify seasonal parameters


1. linear regression with the `lm` function, e.g. for a single periodic component:<br> `m <- lm(x ~ sin(2 * pi * t / N) + cos(2 * pi * t / N))`
2. nonlinear regression with `nls` [(allows to identify the period as nonlinear parameter)]{.gray}
3. classical Fourier analysis:

\begin{align}
     a_0 &= \bar{x}\\
 a_{N/2} &= \sum_{t=1}^{N}(-1)^tx_t/N\\
 a_p     &= 2 \frac{\sum_{t=1}^{N} x_t \cos(2 \pi p t/N)}{N}\\
 b_p     &= 2 \frac{\sum_{t=1}^{N} x_t \sin(2 \pi p t/N)}{N}
\end{align} 

4. Fast Fourer transform (FFT) using complex numbers<br>
  [used in signal processing (CD and MP3 players, mobile phones, ...]{.gray}

## Fast Fourier transform

```{r fft-demo, echo=TRUE}
#| code-fold: true
#| code-summary: "Show the code"
# create an arbitrary time series
set.seed(123)
n   <- 360
t   <- 0:(n-1)
x   <- sin(t*2*pi/n) + rnorm(n, sd = 0.2)
plot(t, x, xlim = c(-10, 370))
  
# perform the FFT
p       <- fft(x)

# invert the FFT for the main period only (keep parameters 1..3)
p[4:n]  <- 0 # set higher order parameters to zero
x_sim      <- fft(p, inverse = TRUE)
lines(t, 2 * Re(x_sim)/n - Re(p[1])/n, col = "blue", lwd = 4)


## perform FFT and invert it for all periods
p       <- fft(x)
p[(n/2):n] <- 0 # set only the "redundant frequencies" to zero
x_sim         <- fft(p, inverse = TRUE)
lines(t, 2 * Re(x_sim)/n - Re(p[1])/n, col = "red", lwd = 1)
```



## Automatic time series decomposition

Function `decompose` implements the classical approach with symmetric moving average.

```{r decompose,echo=TRUE,fig=TRUE,width=10, height=10, fig.align='center'}
manaus_dec <- decompose(manaus)
plot(manaus_dec)
```

* In this data set the seasonal component possesses an additive character.
* In case of a multiplicative components, use `type = "multiplicative"`.


## Automatic time series decomposition II

* function `stl` (seasonal time series decomposition) [@cleveland_stl_1990]
* uses a LOESS filter

```{r stl,fig=TRUE, echo=TRUE,width=10, height=8, fig.align='center'}
manaus_stl <- stl(manaus, s.window=13)
plot(manaus_stl)
```

# ARMA, ARIMA and SARIMA models

<br>

* based on autoregressive (AR) and moving average (MA) terms
* the "I" in ARIMA stands for differencing
* seasonal ARIMA models (SARIMA) consider seasonal dependency,<br>
  e.g. differencing with a lag of 12 months
* very important techniques for analyzing time series and for forecasting
* for the interested: self-study, see @kleiber_applied_2008 or @shumway_time_2019.

## Example: SARIMA model for CO2 in the atmosphere

```{r co2-sarima0, echo=TRUE}
#| code-fold: true
#| code-summary: "Show the code"
## read data directly from NOAA
#co2 <- read_csv("https://gml.noaa.gov/webdata/ccgg/trends/co2/co2_mm_mlo.csv", 
#                skip = 40, show_col_types = FALSE)

## local version
co2 <- read_csv("../data/co2_mm_mlo.csv", skip=40, show_col_types = FALSE)

## year 1958 is incomplete, so let's start with 1959
co2 <- dplyr::filter(co2, year > 1958) 

co2 |> 
  mutate(date=as_date(paste(year, month, 15, sep="."))) |>
  ggplot(aes(date, average)) + 
  geom_line() + ylab(expression(CO[2]~"in the atmosphere (ppm)")) + 
  ggtitle("Mauna Loa Observatory, Hawaii, Monthly Averages") +
  geom_smooth()
```


## Fit SARIMA model to CO2 data
```{r co2-sarima1, echo=TRUE, results='hide'}
#| code-fold: true
#| code-summary: "Show the code"
library(astsa)

## convert to a time series object
co2ts <- ts(co2$average, start=1959, frequency = 12)

## fit a SARIMA model and show the results
m <- sarima(co2ts, p = 1, d = 1, q = 1, P = 0, D = 1, Q = 1, S = 12)
```
Mathematical details can be found in @shumway_time_2019.


## SARIMA-forecast of CO2 

```{r co2-sarima2, echo=TRUE}
#| code-fold: true
#| code-summary: "Show the code"
m <- sarima.for(co2ts, n.ahead = 12 * 27, 
           p = 1, d = 1, q = 1, P = 0, D = 1, Q = 1, S = 12, 
           plot.all = TRUE)
```
Mathematical details can be found in @shumway_time_2019.

# Identification of structural breaks


## Identification of structural breaks

```{r nile-intro, fig.align='center', fig.width=10, fig.height=3}
par(mar=c(4,5,2,1))
library(strucchange)
data("Nile")
bp.nile <- breakpoints(Nile ~ 1)
fm1 <- lm(Nile ~ breakfactor(bp.nile,  breaks = 1))
plot(Nile, las=1, ylab=expression(10^~m^3), main="Discharge of the river Nile")
lines(ts(fitted(fm1),  start = 1871),  col = 4, lwd=2)
```


**Structural break**

If one or more statistical parameters are not constant over the whole length of a
time series it is called a structural break. For instance a location parameter
(e.g. the mean), a trend or another distribution parameter (such as variance or
covariance) may change.

<br>

**Testing for structural breaks**

The package **strucchange** implements a number of tests
for identification of structural changes or parameter instability of time 
series. Generally, two approaches are available: fluctuation tests and
F-based tests. Fluctuation tests try to detect the structural instability with
cumulative or moving sums (CUSUMs and MOSUMs).

## The Nile data set

The Nile data set contains
measurements of annual discharge of the Nile at Aswan from 1871 to 1970
[(see help page ?Nile for data source):]{.gray}

```{r nile, fig=TRUE, echo=TRUE, fig.align='center'}
library(strucchange)
data("Nile")
plot(Nile)
```


## Test for existence of structural break

* Are there are periods with different flow?
* use of OLS-CUSUM (ordinary least squares, cumulative sum)
 or MOSUM tests (moving average sum)
* `efp` = empirical fluctuation process,`sctest` = structural change test


```{r ocus, fig=TRUE, echo=TRUE, fig.align='center'}
ocus <- efp(Nile ~ 1, type = "OLS-CUSUM")
plot(ocus)
sctest(ocus)
```


## Breakpoint analysis

<br>

**Purpose**

* find structural breaks with respect to a specified linear model
* identify their number and location
* very general approach, allows different linear models, covariates, ...
* we show the most simplest case here

**Method**

* employs a BIC-based model selection technique
* Which model is the best?

**See also**

* @Bai2003, @Zeileis2002, @Zeileis2003.



## Remember: model selection techniques

<br>

* *model selection* is a modern alternative to $p$-value based testing
* Based on the principle of parsimony:
    * Models with more parameters fit better, but may contain unnecessary factors.
    * $\rightarrow$ Find an **optimal compromise** between model fit and complexity.
    * Keep it as simple as possible, but not simpler.

* Multiple model candidates for the same phenomenon:
    * [Full model]{.blue}: includes all potential factors.
    * Several [reduced models]{.blue}, derived from the full model.
    * [Null model]{.blue} without explanatory factors.

* Select [minimal adequate model]{.blue}  (= "optimal", "most parsimonious").

More, see @JohnsonOmland2004



## Model selection with AIC and BIC

<br>

Models with more parameters fit better $\leftrightarrow$ but more parameters ($k$) are "more complex".


* Goodness of fit can be measured with likelihood $L$<br>
  [(how likely are the data given a specific model).]{.gray}
* Log likelihood: makes the problem additive.
* Penalty: penalises number of parameters (e.g. $2 k$)
* AIC (Akaike Information Criterion):

$$
 AIC = - 2 \ln(L) + 2 k
$$

* Alternatively: BIC (Bayesian Information Criterion),
 considers sample size ($n$):
 
$$
 BIC = - 2 \ln(L) + k \cdot \ln(n)
$$


The model with smallest AIC (resp. BIC) is considered as "optimal" model.



## Breakpoint analysis

<br>

```{r fig=FALSE, echo=TRUE}
bp.nile <- breakpoints(Nile ~ 1)
summary(bp.nile)
```

**Notes**

* computationally intensive
* consider to adapt model and parameter `h`, see help page

## Likelihood profile: $\rightarrow$ optimal number of breaks?

```{r bp-nile, fig=TRUE, echo=TRUE, fig.align='center'}
bp.nile <- breakpoints(Nile ~ 1)
plot(bp.nile)
```


* RSS (residual sum of squares) shows goodness of fit. The smaller the better.
* Penalty = 2 $\cdot$ number of breakpoints (not shown).
* Minimum BIC indicates optimal model.


## Plot breakpoints and confidence interval

```{r ci-nile, fig=TRUE, echo=TRUE, fig.align='center'}
## fit null hypothesis model and model with 1 breakpoint
fm0 <- lm(Nile ~ 1)
fm1 <- lm(Nile ~ breakfactor(bp.nile,  breaks = 1))
plot(Nile)
lines(ts(fitted(fm0),  start = 1871),  col = 3)
lines(ts(fitted(fm1),  start = 1871),  col = 4)
lines(bp.nile)

## confidence interval
ci.nile <- confint(bp.nile)
#ci.nile  # output numerical results
lines(ci.nile)
```

## Test significance of breakpoints

<br>

**Likelihood ratio test**

```{r fig=FALSE, echo=TRUE}

anova(fm0, fm1)
```

**BIC based comparison**

```{r fig=FALSE, echo=TRUE}
BIC(fm0, fm1)
```

**AIC based comparison**

```{r fig=FALSE, echo=TRUE}
AIC(fm0, fm1)
```


$\rightarrow$ model with structural break in 1898 is 
significantly better than the null model.


## Diagnostics

<br>


```{r nile-diagnostics, echo=TRUE, fig.align='center'}
par(mfrow = c(2, 2))
acf(residuals(fm0))     # model without breaks
acf(residuals(fm1))     # model with 1 breakpoint
qqnorm(residuals(fm0))
qqnorm(residuals(fm1))
```

<!--
#spectrum(residuals(fm0), log = "no",  spans = c(5, 5))
#spectrum(residuals(fm1), log = "no",  spans = c(5, 5))
-->


## References

<br>

