---
subtitle: "Applied Statistics -- A Practical Course"
title: "08-Nichtlineare Regression"
date:   "`r Sys.Date()`"
--- 


## Viele Phänomene in der Natur sind nichtlinear.

<br>


**Linear oder nichtlinear?**

* Einige nichtlineare Probleme können mit linearen Methoden gelöst werden:
    * z.B. Polynome oder periodische (Sinus- und Cosinus-) Funktionen,
* andere können durch Transformationen angepasst werden.

**Beispiel**

$$y = a \cdot x^b$$
kann umgeformt werden in:

$$\ln(y) = \ln(a) + b \cdot \ln(x)$$

* Aber: Eine Linearisierung transformiert auch die Residuen. 
  → Das kann zu verzerrten (biased) Schätzungen führen



## Linearisierung oder direkte nichtlineare Anpassung?

<br>

**Linearisierende Transformationen**

* Logarithmus, Quadratwurzel, Kehrwerte ...
* können angewandt werden, wenn die Restvarianz homogen bleibt (oder wird).
* In einigen Fällen wird die Residuenvarianz sogar stabilisiert,
* in anderen Fällen führt eine Transformation zu Heteroskedastizität<br>
  $\Rightarrow$ **verzerrte** (biased) Regressionsparameter.

**Nichtlineare Regression**

* sehr flexible, fast beliebige benutzerdefinierte Funktionen
* keine Transformation erforderlich
* aber: erfordert **iterative** Optimierungsmethoden
* theoretisch können nur lokale Optima gefunden werden
* Parameter sind nicht in allen Fällen **identifizierbar**


## Nichtlineare Regression

<br>

**Iterative Suche nach dem Minimum der Summe der kleinsten Quadrate**

![](../img/optimization.png)

## Wie lässt sich das globale Minimum finden?

<br>

**Anpassungsgüte:** Summe der Residuenquadrate $\mathbf{RSS}$:

$$
\mathbf{RSS} = \sum_{i=1}^n\left(y_i- f(\mathbf x_i, \mathbf p)\right)^2 = \min !
$$

mit 

* $y$: abhängige Variable, 
* $\mathbf x$: Matrix der unabhängigen Variablen
* $\mathbf p$: Parametervektor
* $n$: Stichprobenumfang

**Verwendung eines iterativen Lösers** $\rightarrow$ siehe nächste Folien



## Nichtlineares Bestimmtheitsmaß $r^2$

<br>

* misst den Anteil der erklärten Varianz
* kann aus den Rest- und Gesamtvarianz berechnet werden

$$
r^2 = 1 - \frac{\text{Varianz der Residuen}}{\text{Varianz der } y\text{-Werte}} = 1 - \frac{s^2_\varepsilon}{s^2_y}
$$


* identisch mit der Nash-Sutcliffe-Effizienz (NSE), einem Standard-Gütemaß der Hydrologie
* Nichtlineares $r^2$ steht in keinem Zusammenhang mit dem Korrelationskoeffizienten.
* Weitere Gütemaße: MSE, RMSE, bias, $r_P$, $r_S$

**Verwechslungsgefahr** 

* Manchmal wird $r^2$ als Quadrat der Korrelation zwischen beobachteten und modellierten Werten definiert.
* Diese Version ignoriert einen möglichen **bias** $\rightarrow$ keine Aussage zu erklärten Varianz!

## Allgemeines Prinzip von Optimierungsalgorithmen

<br>

Das Minimum der Residuenquadrate (RSS) wird durch [Iteration]{.blue} gesucht.

1. Beginne mit einer Anfangsschätzung für den Parametersatz.
2. Versuche, einen Parametersatz mit niedrigerer RSS zu finden.
3. Ist der neue Parametersatz besser?
    * Nein: Verwerfe die neuen Parameter und gehe zu 2
    * Ja: Akzeptiere die neuen Parameter und gehe zu 4
4. Ist der neue Parametersatz gut genug?
    * Nein: gehe zu 2
    * Ja: gehe zu 5
5. Parametersatz gefunden.

**Hinweise**

* Oft wird ein **Konvergenzkriterium** verwendet (z.B. nur noch kleine Änderung der RSS).  
* Der Algorithmus kann in ein **lokales Minimum** laufen.  
* Die **Anfangsschätzung** ist entscheidend!

## Deterministische Methoden

<br>

:::{.column width="64%"}
**Gradient Descent**

* einen Schritt in die Richtung des steilsten Abstiegs gehen
* $\rightarrow$ relativ einfach
* $\rightarrow$ relativ robust für „kompliziertere“ Probleme

**Newton-Methode**

* quadratische Approximation der RSS-Funktion
* Versuche, das Minimum zu schätzen
* $\rightarrow$ berücksichtigt die Krümmung (Hesse-Matrix)
* $\rightarrow$ ist schneller für „gut funktionierende“ Probleme
* mehrere Versionen:<br> quasi-Newton, Gauss-Newton, L-BFGS, ...

**Levenberg-Marquardt** 

* interpoliert zwischen Gauß-Newton und Gradient Descent.

:::

:::{.column width="34%"}

![](../img/newton_optimization_vs_grad_descent.svg)
<small>
Die Newton-Methode ([rot]{.red}) nutzt Krümmungsinformationen, um schneller zu konvergieren als der Gradient Descent ([grün]{.neongreen}).<br> 
Quelle: [Wikipedia](https://en.wikipedia.org/wiki/Newton's_method_in_optimization).
</small>
:::


## Stochastische Methoden

<br>

Anwendung statistischer Prinzipien (Zufallssuche)

**Klassische Methoden**

* Simulated Annealing: simuliert die Erwärmung und Abkühlung von Materie $\rightarrow$ „Kristallisationsprozess“.
* Kontrollierte Zufallssuche [@Price1977, @Price1983]

**Evolutionäre Algorithmen**

* Analogie zur Genetik: Mutation und Selektion
* verfolgt eine „Population“ von mehreren Parametersätzen parallel
* Informationsaustausch („Crossover“) zwischen Parametersätzen
* $\rightarrow$ für komplizierte Probleme mit einer großen Anzahl von Parametern
* heutzutage in Microsoft Excel und LibreOffice Calc eingebaut

Stochastische Methoden sind besonders nützlich, wenn das Optimierungsproblem **nicht glatt** oder **nicht konvex** ist oder **viele lokale Minima** besitzt.


# Beispiele für nichtlineare Regression


## Beispiele


<br>

* **Enzymkinetik**  
  - Michaelis-Menten-Gleichung: $V = \frac{V_{\max} \cdot S}{K_m + S}$
  - Beschreibt die Reaktionsgeschwindigkeit in Abhängigkeit von der Substratkonzentration

* **Wachstum von Organismen**  
  - Logistisches Wachstum: $N(t) = \frac{K}{1 + A \cdot e^{-rt}}$
  - Beschreibt S-förmiges Wachstum mit Sättigung

* **Kalibrierung komplexer Modelle** in vielen Disziplinen:  
  - Chemie, Biologie, Ingenieurwesen, Finanzwirtschaft, Sozialwissenschaften  
  - Hydrologie, Hydrophysik, Grundwasser, Abwasser, Wasserqualität

## Enzymkinetik

<br>

Michaelis-Menten-Funktion:

$$
v = v_{max} \frac{S}{k_m + S}
$$

```{r enzymekinetics, height=6,width=10}
S <-c(25, 25, 10, 10, 5, 5, 2.5, 2.5, 1.25, 1.25)
V <-c(0.0998, 0.0948, 0.076, 0.0724, 0.0557, 
      0.0575, 0.0399, 0.0381, 0.017, 0.0253)
model_fit<-nls(V ~ SSmicmen(S, Vm, K))

par(mfrow=c(1,2), las=1, lwd=2)
## Lineweaver-Burk
x <- 1/S; y <- 1/V

plot(x, y, xlab="1/S", ylab="1/V", xlim=c(-0.2,1), ylim=c(0, 80), pch=16,
  main="Linearisation\n(Lineweaver-Burk)")
abline(h=0, lwd=1, col="grey")
abline(v=0, lwd=1, col="grey")
m <- lm(y ~ x)
abline(m, col = "red")
Vm_l <- 1/coef(m)[1]
Km_l <- coef(m)[2] * Vm_l
#legend("topleft", c("vmax = 1/intercept", "km = slope * vmax"), 
#        box.lwd=1, bg="white")
text(0.35, 75, "1/V = 1/vmax + km / vmax * S")

## retransformed, both together
plot(S, V, xlim = c(0, max(S)),ylim=c(0, max(V)), pch=16, main="No Transformation")
x1 <-seq(0, 25, length=100)
lines(x1, Vm_l * x1 / (Km_l + x1), col="red")
lines(x1, predict(model_fit, data.frame(S=x1)), col="blue")
legend("bottomright", legend=c("linearisation", "nonlinear"), 
       box.lwd=1, lwd=2, col=c("red", "blue"))
text(15, 0.04, "V = S * vmax / (km + S)")
```


## Linearisierung oder (echte) nichtlineare Regression?

<br>

**Linearisierende Transformation**

[>] Angemessen, wenn die Transformation die Homogenität der Varianzen verbessert.<br>
[+] Schnell, einfach und leicht zu implementieren.<br>
[+] Analytische Lösung liefert das globale Optimum.<br>
[-] Nur eine begrenzte Anzahl von Funktionen kann angepasst werden.<br>
[-] Kann zu einer falsch transformierten Fehlerstruktur und verzerrten Ergebnissen führen.


**Nichtlineare Regression**

[>] Geeignet, wenn die Fehlerstruktur homogen ist oder keine analytische Lösung existiert.<br>
[+] Ermöglicht Anpassung beliebiger Funktionen -- sofern die Parameter identifizierbar sind.<br>
[-] Erfordert sorgfältige Auswahl von Startwerten. <br>
[-] Rechenintensiv – hoher Zeitaufwand. <br>
[-] Globales Optimum ist **nicht garantiert** – nur lokales Optimum möglich.

<br>

**Empfehlung**  
 
* Verwende **nichtlineare Regression**, wenn möglich – sie ist **robuster und realistischer**.  
* Nutze Linearisierung nur, wenn sie **wirklich sinnvoll** ist (z.B. Varianzstabilisierung).


## Nichtlineare Regression in R: einfache exponentielle Regression

:::{.column width="49%" .add-space}

**Modell anpassen**

```{r nlregexp1, echo=TRUE}
# Beispieldaten
x <- 1:10
y <- c(1.6, 1.8, 2.1, 2.8, 3.5, 
       4.1, 5.1, 5.8, 7.1, 9.0)

# Anfangsparameter für den Optimierer
pstart <- c(a = 1, b = 1)

# nichtlineare kleinste Quadrate
fit <- nls(y ~ a * exp(b * x), start = pstart)
summary(fit)
```
:::

:::{.column width="44%"}

**Plotte Ergebnisse**

```{r nlregexp, echo = TRUE, eval = TRUE, fig.width=6, fig.height=6}

# zusätzliche x-Werte, für eine geglättete Kurve
x1 <- seq(1, 10, 0.1)
y1 <- predict(fit, data.frame(x = x1))
plot(x, y)
lines(x1, y1, col = "red")
```
:::

## Angepasste Parameter

<br>

```{r nlregexp2}
summary(fit)
```

* **Estimate:** die angepassten Parameter
* **Std. Error:** $s_{\bar{x}}$: zeigt die Zuverlässigkeit der Parameter
* t- und p-Werte: Vorsicht bei der Interpretation! 
* In der nichtlinearen Regression sind p-Werte nicht verlässlich – sie basieren auf Approximationen.
* Ein „nicht-signifikanter“ Parameter kann strukturrelevant sein.



**Bestimmtheitsmaß $r^2 = 1-\frac{s^2_\varepsilon}{s^2_y}$**

```{r echo=TRUE}
(Rsquared <- 1 - var(residuals(fit))/var(y))
```


## Michaelis-Menten-Kinetik

<br>

:::{.column width="46%" .add-space}

**Code**

```{r mmenten, echo=TRUE, eval = FALSE}
S <-c(25, 25, 10, 10, 5, 5, 2.5, 2.5, 1.25, 1.25)
V <-c(0.0998, 0.0948, 0.076, 0.0724, 0.0557,
      0.0575, 0.0399, 0.0381, 0.017, 0.0253)

## Michaelis-Menten-Kinetik
f <- function(S, Vm, K) { Vm * S/(K + S) }

pstart <- c(Vm = 0.1, K = 5)
model_fit <- nls(V ~ f(S, Vm, K), start = pstart)
summary(model_fit)

plot(S, V, xlim = c(0, max(S)), ylim = c(0, max(V)))
x1 <-seq(0, 25, length = 100)
lines(x1, predict(model_fit, data.frame(S = x1)), col = "red")
```

<br>
**Bestimmtheitsmaß**

```{r echo=TRUE}
(1 - var(residuals(model_fit))/var(V))
```


:::


:::{.column width="46%"}
**Ergebnis**
```{r mmenten, fig.width=6, fig.height=4}
```
:::


## Michaelis-Menten-Kinetik mit Selbststart

* Funktion `SSmicmen` bestimmt Startparameter automatisch.
* Nur wenige Selbststartfunktionen in R verfügbar.

::::{.column width="49%"}
**Code**


```{r mmselfstart, echo=TRUE, eval = FALSE}
S <- c(25, 25, 10, 10, 5, 5, 2.5, 2.5, 1.25, 1.25)
V <- c(0.0998, 0.0948, 0.076, 0.0724, 0.0557,
       0.0575, 0.0399, 0.0381, 0.017, 0.0253)

model_fit <- nls(V ~ SSmicmen(S, Vm, K))

plot(S, V, xlim = c(0, max(S)), ylim = c(0, max(V)))
x1 <-seq(0, 25, length = 100)
lines(x1, predict(model_fit, data.frame(S = x1)), col="red")
```

**Ergebnisse**

:::{.smallfont}
```{r echo=TRUE}
summary(model_fit, correlation=TRUE)
```
:::
::::

:::{.column width="3%"}
:::

::::{.column width="46%"}

**Plot**

```{r mmselfstart, fig.width=6, fig.height=3}
```

:::{.smallfont}
**Anmerkung:** Korrelation der Parameter

* Hohe absolute Korrelationswerte deuten auf die Nichtidentifizierbarkeit von Parametern hin.
* kritischer Wert hängt von den Daten ab
* manchmal können bessere Startwerte oder ein anderer Optimierungsalgorithmus helfen
:::
::::


## Praktische Hinweise

1. **Daten immer plotten!**  
   → Visuelle Inspektion hilft, die Struktur zu erkennen und Startwerte zu finden.

2. **Finde gute Ausgangswerte**  
   → Durch Nachdenken über die Biologie/Physik des Problems  
   → Oder durch Probieren (z.B. mit `plot(S, V)` und schätzen der Kurve)

3. **Vermeide extrem kleine oder große Zahlen**  
   → Reskaliere die Variablen auf Werte zwischen etwa **0.001 und 1000**  
   → Verbessert Stabilität und Konvergenz der Optimierung

4. **Beginne einfach**  
   → Starte mit einer einfachen Funktion  
   → Füge nach und nach Terme und Parameter hinzu – Schritt für Schritt

5. **Interpretiere Signifikanz vorsichtig**  
   → p-Werte in nichtlinearen Modellen sind **nicht verlässlich**  
   → Ein „nicht signifikanter“ Parameter kann **strukturrelevant** sein  
   → Sein Wegfall macht das Modell ungültig – **nicht ausschließen!**

**Wichtig:**  

Die Qualität eines Modells hängt nicht von p-Werten ab – sondern von **Plausibilität, Anpassungsgüte und Vorhersagekraft**.



## Weiterführende Literatur

<br>

* Paket **growthrates** für Wachstumskurven: https://cran.r-project.org/package=growthrates
* Paket **FME** für komplexere Modellanpassungsaufgaben (Identifizierbarkeitsanalyse, eingeschränkte Optimierung, mehrere abhängige Variablen und MCMC): [@FME], https://cran.r-project.org/package=FME 

* Mehr über Optimierung in **R**: https://cran.r-project.org/web/views/Optimization.html

# Anhang



## Lineweaver-Burk-Transformation vs. nichtlineare Anpassung

<br>

**Code der Methodenvergleichsfolie**

:::{.smallfont}
```{r enzymekinetics, eval=FALSE, echo=TRUE}
```
:::

Siehe: [https://en.wikipedia.org/wiki/Lineweaver-Burk_plot](https://en.wikipedia.org/wiki/Lineweaver-Burk_plot)


## Referenzen

<br>

